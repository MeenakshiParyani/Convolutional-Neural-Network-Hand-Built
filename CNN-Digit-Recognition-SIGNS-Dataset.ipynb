{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1 (70pts) Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One hot encode the labels\n",
    "def one_hot_encode(y):\n",
    "    targets = np.array(np.unique(y)).reshape(-1)\n",
    "    enc = np.eye(len(targets))[y]\n",
    "    print(enc)\n",
    "    return enc\n",
    "\n",
    "# Get the loss of for the training example\n",
    "def cross_entropy(Y, Yhat):\n",
    "    m= Y.shape[1]\n",
    "    eps=1e-15\n",
    "    Yhat = np.clip(Yhat, eps, 1-eps)\n",
    "    loss = np.multiply(np.log(Yhat),Y) + np.multiply((1.-Y), np.log(1. - Yhat))\n",
    "    loss = np.sum(loss)\n",
    "    cost = -1./m * np.sum(loss)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while forward propagation\n",
    "# Purpose of this method is to do squishing on the linear function\n",
    "def sigmoid(z):\n",
    "    print(\"Before sigmoid---\")\n",
    "    print(z)\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    print(\"After sigmoid---\")\n",
    "    print(sigmoid)\n",
    "    return sigmoid\n",
    "\n",
    "# Applying Sigmoid Activation function to the hidden layer outputs used while backward propagation to get gradients\n",
    "# Purpose of this method is to do undo the squishing on the linear function\n",
    "def sigmoid_prime(z):\n",
    "    inv = (np.exp(-z))/(np.power((1+np.exp(-z)),2))\n",
    "    return inv\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(z,0)\n",
    "    \n",
    "def relu_prime(z):\n",
    "    return np.where(z < 0, 0.0, 1.0)\n",
    "\n",
    "# Softmax activation function to get the probablity of the classes\n",
    "def softmax(z):\n",
    "    softMax = (np.exp(z) / np.sum(np.exp(z),axis=0))\n",
    "    softMax = np.matrix(np.argmax(softMax,axis=0)).T\n",
    "    return softMax\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    ## Pads the height and width and breadth only by 'pad' columns using constant value, 4 dimensional padding\n",
    "    padded_array = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0,0)), 'constant', constant_values = (0,0))\n",
    "    print(\"Dimension after padding\" + str(padded_array.shape))\n",
    "    return padded_array\n",
    "\n",
    "# Convolution of a slice\n",
    "def convolute_slice(a_slice, W, b):\n",
    "    conv = np.multiply(a_slice, W)\n",
    "    z = np.sum(conv) + float(b)\n",
    "    return z\n",
    "\n",
    "# Calculate the coordinates of the slice\n",
    "def get_slice_coordinates(stride, f, w, h):\n",
    "    vert_start = h * stride\n",
    "    vert_end = vert_start + f\n",
    "    horiz_start = w * stride\n",
    "    horiz_end = horiz_start + f\n",
    "    return vert_start, vert_end, horiz_start, horiz_end\n",
    "\n",
    "# Flatten the array\n",
    "def flatten(A):\n",
    "    (m, nH, nW, nC) = A.shape\n",
    "    print('Shape before flattening is----')\n",
    "    print(A.shape)\n",
    "    A = A.reshape(m, nH * nW * nC)\n",
    "    print('Shape after flattening is----')\n",
    "    print(A.shape)\n",
    "    return A.transpose()\n",
    "\n",
    "def convolute_forward(A_prev, W, b, hyper_params):\n",
    "    # Get the required parameters\n",
    "    m, nH_prev, nW_prev, nC_prev = A_prev.shape\n",
    "    f, f, nC_prev, nC  = W.shape\n",
    "    pad = hyper_params[\"pad\"]\n",
    "    stride = hyper_params[\"stride\"]\n",
    "    print('convolution nH_prev, f, pad, stride---------------')\n",
    "    print(nH_prev, f, pad, stride)\n",
    "    # Calculate the dimensions of input\n",
    "    nH = int((nH_prev -f + 2 * pad)/stride) + 1\n",
    "    nW = int((nW_prev -f + 2 * pad)/stride) + 1\n",
    "    print('convolution m, nH, nW, nC---------------')\n",
    "    print(m, nH, nW, nC)\n",
    "    # Calculate the dimensions of output\n",
    "    Z = np.zeros((m, nH, nW, nC))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    # referring professor's code for forward propagation\n",
    "    for i in range(m):\n",
    "        a_pad_prev = A_prev_pad[i, :, :, :]\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    # Find the coordinates of the current slice\n",
    "                    vert_start, vert_end, horiz_start, horiz_end = get_slice_coordinates(stride, f, w, h)\n",
    "                    \n",
    "                    # Extract the slice from input\n",
    "                    a_slice_prev = a_pad_prev[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n",
    "                    Z[i, h, w, c] = convolute_slice(a_slice_prev, W[:, :, :, c], b[:, :, :, c])\n",
    "                    \n",
    "    cache = (A_prev, W, b, hyper_params)\n",
    "    return Z\n",
    "\n",
    "def pool_forward(A_prev, hyper_params, pooling_method):\n",
    "    # Get the required parameters\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    print('Pooling (m, nH_prev, nW_prev, nC_prev---------------')\n",
    "    print(m, nH_prev, nW_prev, nC_prev)\n",
    "    f = hyper_params[\"f\"]\n",
    "    stride = hyper_params[\"stride\"]\n",
    "    print('Pooling f, stride')\n",
    "    print(f, stride)\n",
    "    # Calculate the dimensions of the output\n",
    "    nH = int(1 + (nH_prev - f) / stride)\n",
    "    nW = int(1 + (nW_prev - f) / stride)\n",
    "    nC = nC_prev\n",
    "    #print(nH, nW, nC)\n",
    "    # Initialize the output matrix\n",
    "    A = np.zeros((m, nH, nW, nC))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    \n",
    "                    # Find the coordinates of the current slice\n",
    "                    vert_start, vert_end, horiz_start, horiz_end = get_slice_coordinates(stride, f, w, h)\n",
    "                    \n",
    "                     # Extract the slice from input\n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Perform the pooling on the slice\n",
    "                    if(pooling_method == \"max\"):\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif(pooling_method == \"avg\"):\n",
    "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
    "    cache = (A_prev, hyper_params)                \n",
    "    assert(A.shape == (m, nH, nW, nC))\n",
    "    return A\n",
    "        \n",
    "             \n",
    "def convolute_backward(dZ, A_prev, W, b, hyper_params):\n",
    "    # Get the input dimensions\n",
    "    (m, nH_prev, nW_prev, nC_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hyper_params[\"stride\"]\n",
    "    pad = hyper_params[\"pad\"]\n",
    "    (m, nH, nW, nC) = dZ.shape\n",
    "    dA_prev = np.zeros((m, nH_prev, nW_prev, nC_prev))                           \n",
    "    dW = np.zeros((f, f, nC_prev, nC))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    A_pad_prev = zero_pad(A_prev, pad)\n",
    "    dA_pad_prev = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        a_pad_prev = A_pad_prev[i,:,:,:]\n",
    "        da_pad_prev = dA_pad_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "\n",
    "                    # Find the coordinates of the current slice\n",
    "                    vert_start, vert_end, horiz_start, horiz_end = get_slice_coordinates(stride, f, w, h)\n",
    "\n",
    "                    # Extract the slice from input\n",
    "                    a_slice = A_pad_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    da_pad_prev[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "    \n",
    "        dA_prev[i, :, :, :] = da_pad_prev[pad:-pad, pad:-pad, :]\n",
    "    assert(dA_prev.shape == (m, nH_prev, nW_prev, nC_prev))\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "# Backward pass for max pooling for a slice\n",
    "def backprop_max_pooling(x):\n",
    "    mask = x==np.max(x)\n",
    "    return mask\n",
    "\n",
    "# Backward pass for avg pooling for a slice\n",
    "def backprop_avg_pooling(dz, slice_shape):\n",
    "    (nH, nW) = slice_shape\n",
    "    avg = dz/(nH * nW) # calculate avg\n",
    "    a = np.ones((nH, nW)) * avg\n",
    "    return a\n",
    "\n",
    "\n",
    "def pool_backward(dA, A_prev, hyper_params, pooling_method):\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    m, nH_prev, nW_prev, nC_prev = A_prev.shape\n",
    "    m, nH, nW, nC = dA.shape\n",
    "    dA_prev = np.zeros((m, nH_prev, nW_prev, nC_prev))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        for h in range(nH):\n",
    "            for w in range(nW):\n",
    "                for c in range(nC):\n",
    "                    \n",
    "                    # Find the coordinates of the current slice\n",
    "                    vert_start, vert_end, horiz_start, horiz_end = get_slice_coordinates(stride, f, w, h)\n",
    "                    \n",
    "                    if(pooling_method == \"max\"):\n",
    "                        a_slice_prev = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]                  \n",
    "                        mask = backprop_max_pooling(a_slice_prev)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
    "\n",
    "                    elif(pooling_method == \"avg\"):\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f,f)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += backprop_avg_pooling(da, shape)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    return dA_prev\n",
    "\n",
    "\n",
    "# Forward propagation to calculate yHat by applying activation function twice\n",
    "def forward_propagate(cache, hyper_params, parameters):\n",
    "    \n",
    "    # Layer 1 Convolution\n",
    "    cache[\"Z1c\"] = convolute_forward(cache[\"A0\"], parameters[\"W1c\"], parameters[\"b1c\"], hyper_params[\"c1\"]) # Convolution\n",
    "    cache[\"A1c\"] = relu(cache[\"Z1c\"]) # Relu Activation\n",
    "#     print('-----------------Z1c--------------')\n",
    "#     print(cache[\"Z1c\"])\n",
    "#     print('-----------------A1c--------------')\n",
    "#     print(cache[\"A1c\"])\n",
    "    \n",
    "    # Layer 1 Pooling \n",
    "    cache[\"A1p\"] = pool_forward(cache[\"A1c\"], hyper_params[\"p1\"], pooling_method=\"max\") # Pooling\n",
    "#     print('-----------------A1p--------------')\n",
    "#     print(cache[\"A1p\"])\n",
    "    \n",
    "    # Layer 2 Convolution \n",
    "    cache[\"Z2c\"] = convolute_forward(cache[\"A1p\"], parameters[\"W2c\"], parameters[\"b2c\"], hyper_params[\"c2\"]) # Convolution\n",
    "    cache[\"A2c\"] = relu(cache[\"Z2c\"]) # Relu Activation\n",
    "#     print('-----------------Z2c--------------')\n",
    "#     print(cache[\"Z2c\"])\n",
    "#     print('-----------------A2c--------------')\n",
    "#     print(cache[\"A2c\"])\n",
    "    \n",
    "    # Layer 2 Pooling \n",
    "    cache[\"A2p\"] = pool_forward(cache[\"A2c\"],hyper_params[\"p2\"], pooling_method=\"avg\") # Pooling\n",
    "#     print('-----------------A2p--------------')\n",
    "#     print(cache[\"A2p\"])\n",
    "    \n",
    "    # Flatten the array\n",
    "    cache[\"A3\"] = flatten(cache[\"A2p\"])\n",
    "#     print('-----------------A3--------------')\n",
    "#     print(cache[\"A3\"])\n",
    "\n",
    "    # Fully Connected Layer 4\n",
    "    cache[\"Z4\"] = np.dot(parameters[\"W4\"], cache[\"A3\"]) + parameters[\"b4\"]\n",
    "    cache[\"A4\"] = relu(cache[\"Z4\"]) # Relu Activation \n",
    "#     print('-----------------Z4--------------')\n",
    "#     print(cache[\"Z4\"])\n",
    "#     print('-----------------A4--------------')\n",
    "#     print(cache[\"A4\"])\n",
    "    \n",
    "    # Fully Connected Layer 5\n",
    "    cache[\"Z5\"] = np.dot(parameters[\"W5\"], cache[\"A4\"]) + parameters[\"b5\"]\n",
    "    cache[\"A5\"] = sigmoid(cache[\"Z5\"]) # Sigmoid Activation \n",
    "#     print('-----------------Z5--------------')\n",
    "#     print(cache[\"Z5\"])\n",
    "#     print('-----------------A5--------------')\n",
    "#     print(cache[\"A5\"])\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def backward_propagate(Y, cache, params, hyper_params):\n",
    "    gradients = {}\n",
    "    # Fully Connected Layer 5\n",
    "    m = cache[\"A5\"].shape[1]\n",
    "    dZ5 = cache[\"A5\"] - Y\n",
    "    dW5 = (1./m) * np.dot(dZ2, cache[\"A4\"].transpose())\n",
    "    db5 = (1./m) * np.sum(dZ2, axis=1)\n",
    "    gradients[\"dW5\"] = dW5\n",
    "    gradients[\"db5\"] = db5\n",
    "    \n",
    "    # Fully Connected Layer 4\n",
    "    temp1 = np.dot( cache[\"W5\"].transpose(), dZ5 )\n",
    "    temp2 = relu_prime(cache[\"Z4\"])\n",
    "    dZ4 = np.multiply(temp1 , temp2) # element wise product of same dimension matrices\n",
    "    dW4 = (1./m) * np.dot(dZ4, cache[\"A3\"])\n",
    "    db4 = (1./m) * np.sum(dZ4, axis =1)\n",
    "    gradients[\"dW4\"] = dW4\n",
    "    gradients[\"db4\"] = db4\n",
    "    \n",
    "    # Un-Flaten the list\n",
    "    dP2 = cache[\"A2p\"]\n",
    "    \n",
    "    dA2 = pool_backward(dP2, hyper_params[\"p2\"], pooling_method=\"avg\")\n",
    "    dZ2 = relu_prime(dA2)\n",
    "    dP1, dW2, db2 = convolute_backward(dZ2, cache[\"A2p\"], params[\"W2c\"], params[\"b2c\"], hyper_params[\"c2\"])\n",
    "    gradients[\"dW2\"] = dW2\n",
    "    gradients[\"db2\"] = db2\n",
    "    \n",
    "    \n",
    "    dA1 = pool_backward(dP1, hyper_params[\"p1\"], pooling_method=\"max\")\n",
    "    dZ1 = relu_prime(dA1)\n",
    "    X, dW1, db1 = convolute_backward(dZ1, cache[\"A1p\"], params[\"W1c\"], params[\"b1c\"], hyper_params[\"c1\"])\n",
    "    gradients[\"dW1\"] = dW1\n",
    "    gradients[\"db1\"] = db1\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = np.load('ex5_train_x.npy')\n",
    "y_train = np.load('ex5_train_y.npy')\n",
    "indx = 2\n",
    "plt.imshow(X_train[indx])\n",
    "plt.show()\n",
    "print(\"Digit in the image is \" + str(y_train[indx]))\n",
    "# print(X_train)\n",
    "\n",
    "# Normalize the data\n",
    "X_train_norm = X_train/255 - 0.5\n",
    "# print(X_train_norm)\n",
    "# y_train_encoded = one_hot_encode(y_train)\n",
    "# X_train_padded = zero_pad(X_train, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3 (10pts) Initialize parameters (Weights, bias for each layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(input_channl, conv1_f, channl_1, conv2_f, channl_2, \n",
    "                          input_size, fc1_size, fc2_size):\n",
    "    params = {}\n",
    "    # Convolution Layer 1 Params\n",
    "    params[\"W1c\"] =  np.random.randn(conv1_f, conv1_f, input_channl, channl_1)\n",
    "    params[\"b1c\"] =  np.zeros((1,1,1, channl_1))\n",
    "    \n",
    "    # Convolution Layer 2 Params\n",
    "    params[\"W2c\"] =  np.random.randn(conv2_f, conv2_f, channl_1, channl_2)\n",
    "    params[\"b2c\"] =  np.zeros((1,1,1,channl_2))\n",
    "    \n",
    "    # FC layer 4 params\n",
    "    params[\"W4\"] =  np.random.randn(fc1_size, input_size)\n",
    "    params[\"b4\"] =  np.zeros((fc1_size, 1))\n",
    "    \n",
    "    # FC layer 5 params\n",
    "    params[\"W5\"] =  np.random.randn(fc2_size, fc1_size)\n",
    "    params[\"b5\"] =  np.zeros((fc2_size, 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_hyper_params():\n",
    "    \n",
    "    hyper_params = {}\n",
    "    \n",
    "    # Convolution Layer 1 Hyper Parameters\n",
    "    hyper_params_c1 = {}\n",
    "    hyper_params_c1[\"stride\"] = 2\n",
    "    hyper_params_c1[\"pad\"] = 1\n",
    "    \n",
    "    # Pooling Layer 1 Hyper Parameters\n",
    "    hyper_params_p1 = {}\n",
    "    hyper_params_p1[\"stride\"] = 1\n",
    "    hyper_params_p1[\"pad\"] = 0\n",
    "    hyper_params_p1[\"f\"] = 5\n",
    "    \n",
    "    # Convolution Layer 2 Hyper Parameters\n",
    "    hyper_params_c2 = {}\n",
    "    hyper_params_c2[\"stride\"] = 2\n",
    "    hyper_params_c2[\"pad\"] = 0\n",
    "    \n",
    "    # Pooling Layer 2 Hyper Parameters\n",
    "    hyper_params_p2 = {}\n",
    "    hyper_params_p2[\"stride\"] = 1\n",
    "    hyper_params_p2[\"pad\"] = 0\n",
    "    hyper_params_p2[\"f\"] = 5\n",
    "    \n",
    "    hyper_params[\"c1\"] = hyper_params_c1\n",
    "    hyper_params[\"p1\"] = hyper_params_p1\n",
    "    \n",
    "    hyper_params[\"c2\"] = hyper_params_c2\n",
    "    hyper_params[\"p2\"] = hyper_params_p2\n",
    "    \n",
    "    return hyper_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cache = {}\n",
    "cache[\"A0\"] = X_train_norm\n",
    "params = initialize_parameters(input_channl=3, conv1_f=4, channl_1=8, conv2_f=4, channl_2=16, \n",
    "                          input_size=1296, fc1_size=108, fc2_size=6)\n",
    "hyper_params = initialize_hyper_params()\n",
    "cache = forward_propagate(cache, hyper_params, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# import decimal\n",
    "# m,n = cache[\"Z5\"].shape\n",
    "# for i in range(m):\n",
    "#     for j in range(n):\n",
    "#         z = cache[\"Z5\"][i][j]\n",
    "#         if(z>=0):\n",
    "#             z = np.exp(-z)\n",
    "#             cache[\"Z5\"][i][j] = 1./(1. + z)\n",
    "#         else:\n",
    "#             z = np.exp(z)\n",
    "#             cache[\"Z5\"][i][j] = z/(1. + z)\n",
    "            \n",
    "# print( cache[\"Z5\"])\n",
    "# print( cache[\"A5\"])\n",
    "# y_train_encoded = one_hot_encode(y_train)\n",
    "# # print(y_train_encoded)\n",
    "# # # print(y_train_encoded.shape)\n",
    "# # # print(cache[\"A5\"].transpose())\n",
    "# cost = cross_entropy(cache[\"A5\"].transpose(),y_train_encoded)\n",
    "# print(cost)\n",
    "\n",
    "# # # print(cache[\"Z5\"])\n",
    "# # print()\n",
    "# # answer = 1.0/(1.0+np.exp(-cache[\"Z4\"]))\n",
    "# # print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
